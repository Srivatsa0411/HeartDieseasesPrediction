# ğŸª€ Heart Disease Prediction with XGBoost

This project builds a robust machine learning pipeline to **predict the presence of heart disease** using patient medical data. It leverages advanced preprocessing, detailed exploratory analysis, and a highly tuned XGBoost model to deliver high predictive accuracy.

---

## ğŸš€ Highlights

* âœ… Achieved **F1 Score of 0.91089** on the test set (`test_X.csv`)
* ğŸ“Š Clean and explainable **EDA notebook** included
* ğŸ§ª Robust pipeline using **XGBoost** + `RandomizedSearchCV`
* ğŸ§¼ Smart preprocessing: outlier capping, null handling, and optional SMOTE
* ğŸ” Designed for production use with clean `src/` module layout

---

## ğŸ§  Project Structure

```
heartdiseaseprediction/
â”œâ”€â”€ data/                        # Input data files
â”‚   â”œâ”€â”€ train.csv
â”‚   â””â”€â”€ test_X.csv
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploratory_checks.ipynb  # EDA and feature sanity checks
â”œâ”€â”€ src/                        # Modular codebase
â”‚   â”œâ”€â”€ config.py               # Feature lists
â”‚   â”œâ”€â”€ preprocess.py           # Cleaning and transformation
â”‚   â”œâ”€â”€ model.py                # Model and hyperparameter tuning
â”‚   â””â”€â”€ utils.py                # Evaluation and submission helper
â”œâ”€â”€ y_predict.csv               # Final predictions for test_X
â”œâ”€â”€ requirements.txt            # Dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ““ Notebooks

The notebook `exploratory_checks.ipynb` performs:

* Null value inspection
* Feature distribution plots
* Outlier and anomaly detection
* Correlation heatmap
* Class imbalance visualization

It helped reveal irregular values like **non-positive cholesterol** and guided the data cleaning strategy.

---

## ğŸ§ª Model Pipeline

The core model is implemented using:

* `XGBoostClassifier` with:

  * Grid-tuned hyperparameters
  * Stratified 10-fold cross-validation
* `Pipeline` + `ColumnTransformer` for modular preprocessing
* Optional `SMOTE` block (currently disabled)
* Final model retrained on full dataset before generating predictions

---

## ğŸ›  Tools & Libraries

* Python 3.x
* Pandas, NumPy, Matplotlib, Seaborn
* Scikit-learn
* XGBoost
* Imbalanced-learn (for optional SMOTE)

---

## âœ… Output

The final prediction file `y_predict.csv` contains the model's output on `test_X.csv`.
Model performance:

* **Validation F1 Score**: \~0.91
* Predictions align closely with actual outcomes, demonstrating strong generalization.

---

## ğŸ“Š Evaluation

The evaluation metric for this competition is **Mean F1-Score**. The F1 score, commonly used in information retrieval, measures accuracy using the statistics precision (**p**) and recall (**r**):

* **Precision (p)** = TP / (TP + FP)
* **Recall (r)** = TP / (TP + FN)

The F1 score is the harmonic mean of precision and recall:

```
F1 = 2 * (p * r) / (p + r)
```

This metric balances both false positives and false negatives and is ideal for imbalanced datasets like this one.

---

## ğŸ“Œ How to Run

1. Install dependencies

   ```
   pip install -r requirements.txt
   ```

2. Place your `train.csv` and `test_X.csv` in the `data/` directory.

3. Run:

   ```
   python main.py
   ```

4. Output saved to `y_predict.csv`.

---

## ğŸ™‹ About the Author

This project was independently developed as part of my machine learning portfolio to demonstrate end-to-end data science capabilities â€” from cleaning raw data to deploying high-performing models.
For inquiries, feel free to connect via [LinkedIn](https://www.linkedin.com/in/srivatsa-bhamidipati/) or explore other projects on my GitHub profile.

---
